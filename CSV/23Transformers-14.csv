2021,Vision Transformer for Classification of Breast Ultrasound Images,"Medical ultrasound (US) imaging has become a prominent modality for breast cancer imaging due to its ease-of-use, low-cost and safety. In the past decade, convolutional neural networks (CNNs) have emerged as the method of choice in vision applications and have shown excellent potential in automatic classification of US images. Despite their success, their restricted local receptive field limits their ability to learn global context information. Recently, Vision Transformer (ViT) designs that are based on self-attention between image patches have shown great potential to be an alternative to CNNs. In this study, for the first time, we utilize ViT to classify breast US images using different augmentation strategies. The results are provided as classification accuracy and Area Under the Curve (AUC) metrics, and the performance is compared with the state-of-the-art CNNs. The results indicate that the ViT models have comparable efficiency with or even better than the CNNs in classification of US breast images."
2022,Spatial Transcriptomics Prediction from Histology jointly through Transformer and Graph Neural Networks,"The rapid development of spatial transcriptomics allows for the measurement of RNA abundance at a high spatial resolution, making it possible to simultaneously profile gene expression, spatial locations, and the corresponding hematoxylin and eosin-stained histology images. Since histology images are relatively easy and cheap to obtain, it is promising to leverage histology images for predicting gene expression. Though several methods have been devised to predict gene expression using histology images, they don’t simultaneously include the 2D vision features and the spatial dependency, limiting their performances. Here, we have developed Hist2ST, a deep learning-based model using histology images to predict RNA-seq expression. At each sequenced spot, the corresponding histology image is cropped into an image patch, from which 2D vision features are learned through convolutional operations. Meanwhile, the spatial relations with the whole image and neighbored patches are captured through Transformer and graph neural network modules, respectively. These learned features are then used to predict the gene expression by following the zero-inflated negative binomial (ZINB) distribution. To alleviate the impact by the small spatial transcriptomics data, a self-distillation mechanism is employed for efficient learning of the model. Hist2ST was tested on the HER2-positive breast cancer and the cutaneous squamous cell carcinoma datasets, and shown to outperform existing methods in terms of both gene expression prediction and following spatial region identification. Further pathway analyses indicated that our model could reserve biological information. Thus, Hist2ST enables generating spatial transcriptomics data from histology images for elucidating molecular signatures of tissues."
2023,Vision-Transformer-Based Transfer Learning for Mammogram Classification,"Breast mass identification is a crucial procedure during mammogram-based early breast cancer diagnosis. However, it is difficult to determine whether a breast lump is benign or cancerous at early stages. Convolutional neural networks (CNNs) have been used to solve this problem and have provided useful advancements. However, CNNs focus only on a certain portion of the mammogram while ignoring the remaining and present computational complexity because of multiple convolutions. Recently, vision transformers have been developed as a technique to overcome such limitations of CNNs, ensuring better or comparable performance in natural image classification. However, the utility of this technique has not been thoroughly investigated in the medical image domain. In this study, we developed a transfer learning technique based on vision transformers to classify breast mass mammograms. The area under the receiver operating curve of the new model was estimated as 1 ± 0, thus outperforming the CNN-based transfer-learning models and vision transformer models trained from scratch. The technique can, hence, be applied in a clinical setting, to improve the early diagnosis of breast cancer."
2022,Application of Transfer Learning and Ensemble Learning in Image-level Classification for Breast Histopathology,"Background: Breast cancer has the highest prevalence in women globally. The classification and diagnosis of breast cancer and its histopathological images have always been a hot spot of clinical concern. In Computer-Aided Diagnosis (CAD), traditional classification models mostly use a single network to extract features, which has significant limitations. On the other hand, many networks are trained and optimized on patient-level datasets, ignoring the application of lower-level data labels. Method: This paper proposes a deep ensemble model based on image-level labels for the binary classification of benign and malignant lesions of breast histopathological images. First, the BreaKHis dataset is randomly divided into a training, validation and test set. Then, data augmentation techniques are used to balance the number of benign and malignant samples. Thirdly, considering the performance of transfer learning and the complementarity between each network, VGG16, Xception, ResNet50, DenseNet201 are selected as the base classifiers. Result: In the ensemble network model with accuracy as the weight, the image-level binary classification achieves an accuracy of $98.90\%$. In order to verify the capabilities of our method, the latest Transformer and Multilayer Perception (MLP) models have been experimentally compared on the same dataset. Our model wins with a $5\%-20\%$ advantage, emphasizing the ensemble model's far-reaching significance in classification tasks. Conclusion: This research focuses on improving the model's classification performance with an ensemble algorithm. Transfer learning plays an essential role in small datasets, improving training speed and accuracy. Our model has outperformed many existing approaches in accuracy, providing a method for the field of auxiliary medical diagnosis."
2021,Leveraging information in spatial transcriptomics to predict super-resolution gene expression from histology images in tumors,"Recent developments in spatial transcriptomics (ST) technologies have enabled the profiling of transcriptome-wide gene expression while retaining the location information of measured genes within tissues. Moreover, the corresponding high-resolution hematoxylin and eosin-stained histology images are readily available for the ST tissue sections. Since histology images are easy to obtain, it is desirable to leverage information learned from ST to predict gene expression for tissue sections where only histology images are available. Here we present HisToGene, a deep learning model for gene expression prediction from histology images. To account for the spatial dependency of measured spots, HisToGene adopts Vision Transformer, a state-of-the-art method for image recognition. The well-trained HisToGene model can also predict super-resolution gene expression. Through evaluations on 32 HER2+ breast cancer samples with 9,612 spots and 785 genes, we show that HisToGene accurately predicts gene expression and outperforms ST-Net both in gene expression prediction and clustering tissue regions using the predicted expression. We further show that the predicted super-resolution gene expression also leads to higher clustering accuracy than observed gene expression. Gene expression predicted from HisToGene enables researchers to generate virtual transcriptomics data at scale and can help elucidate the molecular signatures of tissues."
2023,Automatic BI-RADS Classification of Breast Magnetic Resonance Medical Records Using Transformer-Based Models for Brazilian Portuguese,"This chapter aims to present a classification model for categorizing textual clinical records of breast magnetic resonance imaging, based on lexical, syntactic and semantic analysis of clinical reports according to the Breast Imaging-Reporting and Data System (BI-RADS) classification, using Deep Learning and Natural Language Processing (NLP). The model was developed from transfer learning based on the pre-trained BERTimbau model, BERT model (Bidirectional Encoder Representations from Transformers) trained in Brazilian Portuguese. The dataset is composed of medical reports in Brazilian Portuguese classified into six categories: Inconclusive; Normal or Negative; Certainly Benign Findings; Probably Benign Findings; Suspicious Findings; High Risk of Cancer; Previously Known Malignant Injury. The following models were implemented and compared: Random Forest, SVM, Naïve Bayes, BERTimbau with and without finetuning. The BERTimbau model presented better results, with better performance after finetuning."
2023,Breast cancer diagnosis through knowledge distillation of Swin transformer-based teacher–student models,"Abstract: Breast cancer is a significant global health concern, emphasizing the crucial need for a timely and accurate diagnosis to enhance survival rates. Traditional diagnostic methods rely on pathologists analyzing whole-slide images (WSIs) to identify and diagnose malignancies. However, this task is complex, demanding specialized expertise and imposing a substantial workload on pathologists. Additionally, existing deep learning models, commonly employed for classifying histopathology images, often need enhancements to ensure their suitability for real-time deployment on WSI, especially when trained for small regions of interest (ROIs). This article introduces two Swin transformer-based architectures: the teacher model, characterized by its moderate size, and the lightweight student model. Both models are trained using a publicly available dataset of breast cancer histopathology images, focusing on ROIs with varying magnification factors. Transfer learning is applied to train the teacher model, and knowledge distillation (KD) transfers its capabilities to the student model. To enhance validation accuracy and minimize the total loss in KD, we employ the state–action–reward–state–action (SARSA) reinforcement learning algorithm. The algorithm dynamically computes temperature and a weighting factor throughout the KD process to achieve high accuracy within a considerably shorter training timeframe. Additionally, the student model is deployed to analyze malignancies in WSI. Despite the student model being only one-third the size and flops of the teacher model, it achieves an impressive accuracy of 98.71%, slightly below the teacher’s accuracy of 98.91%. Experimental results demonstrate that the student model can process WSIs at a throughput of 1.67 samples s"
2023,Classification of Melanoma Skin Cancer Based on Transformer Deep Learning Model,"An increasing number of genetic and metabolic anomalies have been determined to lead to cancer, which is generally fatal. Cancerous cells may spread to any body part, which can be life-threatening. Skin cancer is significant cancer, and its frequency is increasing worldwide. The main subtypes of skin cancer are squamous and basal cell carcinomas and melanoma. The deep learning methods were used to detect the two primary types of tumours, malignant and benign, by using the MELANOMA dataset. The proposed system utilizes a convolutional neural network (CNN), transformer, and InceptionV3 architecture to learn and extract meaningful features from skin lesion images. The CNN model was trained on a large dataset of dermoscopic images of melanoma and benign lesions. The transformer model in deep learning refers to a neural network architecture based on the transformer architecture specifically designed for image classification tasks. Inception is an image recognition model that has been shown to attain greater than 78.1% accuracy on the ImageNet dataset."
2022,Transformer-based deep learning integrates multi-omic data with cancer pathways,Highlights:
2023,A hierarchical self‐attention‐guided deep learning framework to predict breast cancer response to chemotherapy using pre‑treatment tumor biopsies,"Background: Pathological complete response (pCR) to neoadjuvant chemotherapy (NAC) has demonstrated a strong correlation to improved survival in breast cancer (BC) patients. However, pCR rates to NAC are less than 30%, depending on the BC subtype. Early prediction of NAC response would facilitate therapeutic modifications for individual patients, potentially improving overall treatment outcomes and patient survival. Purpose: This study, for the first time, proposes a hierarchical self‐attention‐guided deep learning framework to predict NAC response in breast cancer patients using digital histopathological images of pre‐treatment biopsy specimens. Methods: Digitized hematoxylin and eosin‐stained slides of BC core needle biopsies were obtained from 207 patients treated with NAC, followed by surgery. The response to NAC for each patient was determined using the standard clinical and pathological criteria after surgery. The digital pathology images were processed through the proposed hierarchical framework consisting of patch‐level and tumor‐level processing modules followed by a patient‐level response prediction component. A combination of convolutional layers and transformer self‐attention blocks were utilized in the patch‐level processing architecture to generate optimized feature maps. The feature maps were analyzed through two vision transformer architectures adapted for the tumor‐level processing and the patient‐level response prediction components. The feature map sequences for these transformer architectures were defined based on the patch positions within the tumor beds and the bed positions within the biopsy slide, respectively. A five‐fold cross‐validation at the patient level was applied on the training set (144 patients with 9430 annotated tumor beds and 1,559,784 patches) to train the models and optimize the hyperparameters. An unseen independent test set (63 patients with 3574 annotated tumor beds and 173,637 patches) was used to evaluate the framework. Results: The obtained results on the test set showed an AUC of 0.89 and an F1‐score of 90% for predicting pCR to NAC a priori by the proposed hierarchical framework. Similar frameworks with the patch‐level, patch‐level + tumor‐level, and patch‐level + patient‐level processing components resulted in AUCs of 0.79, 0.81, and 0.84 and F1‐scores of 86%, 87%, and 89%, respectively. Conclusions: The results demonstrate a high potential of the proposed hierarchical deep‐learning methodology for analyzing digital pathology images of pre‐treatment tumor biopsies to predict the pathological response of breast cancer to NAC."
2022,Transformer-Based Radiomics for Predicting Breast Tumor Malignancy Score in Ultrasonography,"Breast cancer must be detected early to reduce the mortality rate. Ultrasound images can make it easier for the clinician to diagnose cases of dense breasts. This study presents a deep vision transformer-based approach for predicting breast cancer malignancy scores from ultrasound images. In particular, various state-of-the-art deep vision transformers such as BEiT, CaiT, Swin, XCiT, and Vis-Former are adapted and trained to extract robust radiomics to classify breast tumors in ultrasound images as benign or malignant. The best-performing model is used to predict the malignancy score of each input ultrasound image. Experimental results revealed that the proposed approach achieves promising results for the detection of malignant tumors of the breast on ultrasound images."
2021,Vision Transformer for Classification of Breast Ultrasound Images,"Medical ultrasound (US) imaging has become a prominent modality for breast cancer imaging due to its ease-of-use, low-cost and safety. In the past decade, convolutional neural networks (CNNs) have emerged as the method of choice in vision applications and have shown excellent potential in automatic classification of US images. Despite their success, their restricted local receptive field limits their ability to learn global context information. Recently, Vision Transformer (ViT) designs that are based on self-attention between image patches have shown great potential to be an alternative to CNNs. In this study, for the first time, we utilize ViT to classify breast US images using different augmentation strategies. The results are provided as classification accuracy and Area Under the Curve (AUC) metrics, and the performance is compared with the state-of-the-art CNNs. The results indicate that the ViT models have comparable efficiency with or even better than the CNNs in classification of US breast images."
2023,Vision-Transformer-Based Transfer Learning for Mammogram Classification,"Breast mass identification is a crucial procedure during mammogram-based early breast cancer diagnosis. However, it is difficult to determine whether a breast lump is benign or cancerous at early stages. Convolutional neural networks (CNNs) have been used to solve this problem and have provided useful advancements. However, CNNs focus only on a certain portion of the mammogram while ignoring the remaining and present computational complexity because of multiple convolutions. Recently, vision transformers have been developed as a technique to overcome such limitations of CNNs, ensuring better or comparable performance in natural image classification. However, the utility of this technique has not been thoroughly investigated in the medical image domain. In this study, we developed a transfer learning technique based on vision transformers to classify breast mass mammograms. The area under the receiver operating curve of the new model was estimated as 1 ± 0, thus outperforming the CNN-based transfer-learning models and vision transformer models trained from scratch. The technique can, hence, be applied in a clinical setting, to improve the early diagnosis of breast cancer."
2021,Leveraging information in spatial transcriptomics to predict super-resolution gene expression from histology images in tumors,"Recent developments in spatial transcriptomics (ST) technologies have enabled the profiling of transcriptome-wide gene expression while retaining the location information of measured genes within tissues. Moreover, the corresponding high-resolution hematoxylin and eosin-stained histology images are readily available for the ST tissue sections. Since histology images are easy to obtain, it is desirable to leverage information learned from ST to predict gene expression for tissue sections where only histology images are available. Here we present HisToGene, a deep learning model for gene expression prediction from histology images. To account for the spatial dependency of measured spots, HisToGene adopts Vision Transformer, a state-of-the-art method for image recognition. The well-trained HisToGene model can also predict super-resolution gene expression. Through evaluations on 32 HER2+ breast cancer samples with 9,612 spots and 785 genes, we show that HisToGene accurately predicts gene expression and outperforms ST-Net both in gene expression prediction and clustering tissue regions using the predicted expression. We further show that the predicted super-resolution gene expression also leads to higher clustering accuracy than observed gene expression. Gene expression predicted from HisToGene enables researchers to generate virtual transcriptomics data at scale and can help elucidate the molecular signatures of tissues."
2023,A hierarchical self‐attention‐guided deep learning framework to predict breast cancer response to chemotherapy using pre‑treatment tumor biopsies,"Background: Pathological complete response (pCR) to neoadjuvant chemotherapy (NAC) has demonstrated a strong correlation to improved survival in breast cancer (BC) patients. However, pCR rates to NAC are less than 30%, depending on the BC subtype. Early prediction of NAC response would facilitate therapeutic modifications for individual patients, potentially improving overall treatment outcomes and patient survival. Purpose: This study, for the first time, proposes a hierarchical self‐attention‐guided deep learning framework to predict NAC response in breast cancer patients using digital histopathological images of pre‐treatment biopsy specimens. Methods: Digitized hematoxylin and eosin‐stained slides of BC core needle biopsies were obtained from 207 patients treated with NAC, followed by surgery. The response to NAC for each patient was determined using the standard clinical and pathological criteria after surgery. The digital pathology images were processed through the proposed hierarchical framework consisting of patch‐level and tumor‐level processing modules followed by a patient‐level response prediction component. A combination of convolutional layers and transformer self‐attention blocks were utilized in the patch‐level processing architecture to generate optimized feature maps. The feature maps were analyzed through two vision transformer architectures adapted for the tumor‐level processing and the patient‐level response prediction components. The feature map sequences for these transformer architectures were defined based on the patch positions within the tumor beds and the bed positions within the biopsy slide, respectively. A five‐fold cross‐validation at the patient level was applied on the training set (144 patients with 9430 annotated tumor beds and 1,559,784 patches) to train the models and optimize the hyperparameters. An unseen independent test set (63 patients with 3574 annotated tumor beds and 173,637 patches) was used to evaluate the framework. Results: The obtained results on the test set showed an AUC of 0.89 and an F1‐score of 90% for predicting pCR to NAC a priori by the proposed hierarchical framework. Similar frameworks with the patch‐level, patch‐level + tumor‐level, and patch‐level + patient‐level processing components resulted in AUCs of 0.79, 0.81, and 0.84 and F1‐scores of 86%, 87%, and 89%, respectively. Conclusions: The results demonstrate a high potential of the proposed hierarchical deep‐learning methodology for analyzing digital pathology images of pre‐treatment tumor biopsies to predict the pathological response of breast cancer to NAC."
2022,Transformer-Based Radiomics for Predicting Breast Tumor Malignancy Score in Ultrasonography,"Breast cancer must be detected early to reduce the mortality rate. Ultrasound images can make it easier for the clinician to diagnose cases of dense breasts. This study presents a deep vision transformer-based approach for predicting breast cancer malignancy scores from ultrasound images. In particular, various state-of-the-art deep vision transformers such as BEiT, CaiT, Swin, XCiT, and Vis-Former are adapted and trained to extract robust radiomics to classify breast tumors in ultrasound images as benign or malignant. The best-performing model is used to predict the malignancy score of each input ultrasound image. Experimental results revealed that the proposed approach achieves promising results for the detection of malignant tumors of the breast on ultrasound images."
2019,An RDAU-NET model for lesion segmentation in breast ultrasound images,"Breast cancer is a common gynecological disease that poses a great threat to women health due to its high malignant rate. Breast cancer screening tests are used to find any warning signs or symptoms for early detection and currently, Ultrasound screening is the preferred method for breast cancer diagnosis. The localization and segmentation of the lesions in breast ultrasound (BUS) images are helpful for clinical diagnosis of the disease. In this paper, an RDAU-NET (Residual-Dilated-Attention-Gate-UNet) model is proposed and employed to segment the tumors in BUS images. The model is based on the conventional U-Net, but the plain neural units are replaced with residual units to enhance the edge information and overcome the network performance degradation problem associated with deep networks. To increase the receptive field and acquire more characteristic information, dilated convolutions were used to process the feature maps obtained from the encoder stages. The traditional cropping and copying between the encoder-decoder pipelines were replaced by the Attention Gate modules which enhanced the learning capabilities through suppression of background information. The model, when tested with BUS images with benign and malignant tumor presented excellent segmentation results as compared to other Deep Networks. A variety of quantitative indicators including Accuracy, Dice coefficient, AUC(Area-Under-Curve), Precision, Sensitivity, Specificity, Recall, F1score and M-IOU (Mean-Intersection-Over-Union) provided performances above 80%. The experimental results illustrate that the proposed RDAU-NET model can accurately segment breast lesions when compared to other deep learning models and thus has a good prospect for clinical diagnosis."
2022,From machine learning to deep learning: experimental comparison of machine learning and deep learning for skin cancer image segmentation,"Skin lesion analysis is a tedious and challenging task, thus, in this research the suitability of employing machine learning or deep learning approaches for automatic lesion segmentation on dermoscopic skin cancer images is determined. The segmented region can assist clinical experts in understanding the complex lesion structure and internal pattern to find the correct skin cancer type for its early diagnosis and prevention. In this study, I present two methodologies for performing lesion segmentation: machine learning-based optimized K-means with Firefly Algorithm (FA) and Convolutional Neural Network (CNN). In the first model, the FA is hybridized with K-means clustering based on the novel average intensity fitness function to optimize the segmentation map. It is observed in the experimental results that the K-means algorithm may lead to poor results due to the wrong selection of initial centroid value, thus FA is hybridized into it to improve the performance. The second model is an enhanced encoder-decoder-based CNN framework implemented in an end-to-end fashion. These two models are compared to understand whether machine learning or deep learning is suitable to perform medical image segmentation based on a few performance metrics such as accuracy, Intersection over Union (IoU), and DICE index. These methods are evaluated and compared on two benchmark datasets provided by the International Skin Imaging Collaboration (ISIC) named ISIC 2016 [1] and ISIC 2017 [2]. Experimental results showed that the CNN model outperformed the machine learning model with an accuracy difference of 7.98% on ISIC 2016, and 7.32% on ISIC 2017. I concluded from the experimental findings that the deep learning model is more accurate and efficient in segmenting the lesion area as compared to the machine learning model. Thus, findings from this experimental work will be considered for the design of an automatic classification system by incorporating a deep learning-based segmentation approach as a pre-processing step. "
2021,Vision Transformer for Classification of Breast Ultrasound Images,"Medical ultrasound (US) imaging has become a prominent modality for breast cancer imaging due to its ease-of-use, low-cost and safety. In the past decade, convolutional neural networks (CNNs) have emerged as the method of choice in vision applications and have shown excellent potential in automatic classification of US images. Despite their success, their restricted local receptive field limits their ability to learn global context information. Recently, Vision Transformer (ViT) designs that are based on self-attention between image patches have shown great potential to be an alternative to CNNs. In this study, for the first time, we utilize ViT to classify breast US images using different augmentation strategies. The results are provided as classification accuracy and Area Under the Curve (AUC) metrics, and the performance is compared with the state-of-the-art CNNs. The results indicate that the ViT models have comparable efficiency with or even better than the CNNs in classification of US breast images."
2023,Applying Deep Learning Methods for Mammography Analysis and Breast Cancer Detection,"Breast cancer is a serious medical condition that requires early detection for successful treatment. Mammography is a commonly used imaging technique for breast cancer screening, but its analysis can be time-consuming and subjective. This study explores the use of deep learning-based methods for mammogram analysis, with a focus on improving the performance of the analysis process. The study is focused on applying different computer vision models, with both CNN and ViT architectures, on a publicly available dataset. The innovative approach is represented by the data augmentation technique based on synthetic images, which are generated to improve the performance of the models. The results of the study demonstrate the importance of data pre-processing and augmentation techniques for achieving high classification performance. Additionally, the study utilizes explainable AI techniques, such as class activation maps and centered bounding boxes, to better understand the models’ decision-making process."
